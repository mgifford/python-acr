#!/usr/bin/env python3
"""
update_results_index.py

Scan the `results/` directory for candidate runs that are "publishable" and
write `results/index.json` listing the CSV datasets to include on GitHub Pages.

This script also writes a `results/index.local.json` (ignored by Git) that
lists every detected run so local operators can access all datasets without
changing the published index.

Finally, it updates `.gitignore` to whitelist the specific result directories
so they can be committed/published while keeping the rest of `results/` ignored.

Publishable heuristics (a directory is considered publishable if):
- It contains a file named `publish_ready` (use `--mark DIR` to create it).

Usage:
  python scripts/update_results_index.py [--dry-run] [--mark DIR] [--unmark DIR]

Default behavior (no args): generate `results/index.json` and update `.gitignore`.
"""
from __future__ import annotations
import json
import os
import argparse
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
RESULTS = ROOT / 'results'
INDEX_FILE = RESULTS / 'index.json'
LOCAL_INDEX_FILE = RESULTS / 'index.local.json'
GITIGNORE = ROOT / '.gitignore'


def find_publishable_dirs() -> list[Path]:
    if not RESULTS.exists():
        return []
    publishable = []
    for p in sorted(RESULTS.iterdir()):
        if not p.is_dir():
            continue
        if (p / 'publish_ready').exists():
            publishable.append(p)
    return publishable


def find_all_result_dirs() -> list[Path]:
    if not RESULTS.exists():
        return []
    dirs: list[Path] = []
    for p in sorted(RESULTS.iterdir()):
        if not p.is_dir():
            continue
        if p.name.startswith('.'):  # skip system folders such as .trash
            continue
        dirs.append(p)
    return dirs


def pick_dataset_file(run_dir: Path) -> str | None:
    # Prefer issues_thread_analyzed_*.csv, then issues_summarized_*.csv
    candidates = list(run_dir.glob('issues_thread_analyzed_*.csv'))
    if not candidates:
        candidates = list(run_dir.glob('issues_summarized_*.csv'))
    if not candidates:
        # Maybe older naming
        candidates = list(run_dir.glob('issues_raw_*.csv'))
    if not candidates:
        return None
    # Choose the newest candidate by mtime
    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    rel = candidates[0].relative_to(ROOT)
    return str(rel).replace(os.path.sep, '/')


def build_dataset_entries(run_dirs: list[Path]) -> list[str]:
    datasets: list[str] = []
    for d in run_dirs:
        ds = pick_dataset_file(d)
        if ds:
            datasets.append(ds)
    return datasets


def write_index_file(datasets: list[str], destination: Path, dry_run: bool = False) -> list[str]:
    rel_path = destination.relative_to(ROOT)
    if dry_run:
        print(f'Would write {rel_path} with entries:')
        for d in datasets:
            print('  ', d)
        return datasets

    RESULTS.mkdir(parents=True, exist_ok=True)
    with open(destination, 'w', encoding='utf-8') as f:
        json.dump(datasets, f, indent=2)
    print(f'Wrote {rel_path} with {len(datasets)} entries')
    return datasets


def update_gitignore(publishable: list[Path], dry_run: bool = False, verbose: bool = False) -> tuple[list[str], list[Path]]:
    # Ensure .gitignore exists
    lines = []
    if GITIGNORE.exists():
        lines = GITIGNORE.read_text(encoding='utf-8').splitlines()

    header = '# Auto-managed publish whitelist for results (generated by scripts/update_results_index.py)'
    start_idx = None
    end_idx = None
    prev_dirs: list[Path] = []
    # Find previous managed block
    for i, ln in enumerate(lines):
        if ln.strip() == header:
            start_idx = i
            break
    if start_idx is not None:
        # collect existing managed entries
        j = start_idx + 1
        while j < len(lines) and lines[j].strip() != '' and lines[j].startswith('!results/'):
            ln = lines[j].strip()
            # line could be '!results/dir/' or '!results/dir/**'
            if ln.startswith('!results/'):
                p = ln[len('!'):]
                # remove trailing /** or /
                p = p.rstrip('/')
                p = p.replace('/**', '')
                # convert to Path relative to ROOT
                prev_dirs.append(ROOT / p)
            j += 1
        end_idx = j
        # remove existing managed block
        del lines[start_idx:end_idx]

    # Build whitelist entries for current publishable list
    entries = [header]
    current_dirs: list[Path] = []
    for d in publishable:
        rel = d.relative_to(ROOT)
        current_dirs.append(rel)
        # Add both directory and recursive unignore
        entries.append(f'!{rel}/')
        entries.append(f'!{rel}/**')

    new_lines = lines + [''] + entries
    if dry_run:
        print('Would update .gitignore with:')
        for ln in entries:
            print('  ', ln)
        # previous directories and current
        removed = [p for p in prev_dirs if (p.relative_to(ROOT) ) not in current_dirs]
        return entries, removed

    GITIGNORE.write_text('\n'.join(new_lines) + '\n', encoding='utf-8')
    if verbose:
        print(f'Updated {GITIGNORE} with {len(publishable)} whitelist entries')

    # Determine removed directories (previously whitelisted but no longer present)
    removed = [p for p in prev_dirs if (p.relative_to(ROOT)) not in current_dirs]
    return entries, removed


def mark_dir(dir_name: str) -> None:
    d = RESULTS / dir_name
    if not d.exists() or not d.is_dir():
        print('Directory not found under results:', dir_name)
        return
    (d / 'publish_ready').write_text('ready\n', encoding='utf-8')
    print('Marked', d)


def unmark_dir(dir_name: str) -> None:
    d = RESULTS / dir_name
    f = d / 'publish_ready'
    if f.exists():
        f.unlink()
        print('Unmarked', d)
    else:
        print('No publish_ready marker found in', d)


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--dry-run', action='store_true')
    ap.add_argument('--mark', metavar='DIR', help='create publish_ready in results/DIR')
    ap.add_argument('--unmark', metavar='DIR', help='remove publish_ready in results/DIR')
    ap.add_argument('--no-gitignore', action='store_true', help="don't update .gitignore")
    ap.add_argument('--prune', action='store_true', help='move removed whitelisted directories to results/.trash')
    ap.add_argument('--verbose', action='store_true', help='verbose output')
    args = ap.parse_args()

    if args.mark:
        mark_dir(args.mark)
        return
    if args.unmark:
        unmark_dir(args.unmark)
        return

    publishable = find_publishable_dirs()
    print('Found publishable dirs:', [p.name for p in publishable])
    publishable_datasets = build_dataset_entries(publishable)
    write_index_file(publishable_datasets, INDEX_FILE, dry_run=args.dry_run)

    all_dirs = find_all_result_dirs()
    if args.verbose:
        print('Found result dirs (local scope):', [p.name for p in all_dirs])
    local_datasets = build_dataset_entries(all_dirs)
    write_index_file(local_datasets, LOCAL_INDEX_FILE, dry_run=args.dry_run)
    removed_dirs: list[Path] = []
    if not args.no_gitignore:
        entries, removed = update_gitignore(publishable, dry_run=args.dry_run, verbose=args.verbose)
        removed_dirs = removed

    # If prune requested, move removed directories to results/.trash
    if args.prune and removed_dirs:
        trash = RESULTS / '.trash'
        if not args.dry_run:
            trash.mkdir(exist_ok=True)
        for p in removed_dirs:
            if not p.exists():
                if args.verbose:
                    print('Skipping non-existent removed dir', p)
                continue
            target = trash / p.name
            if args.dry_run:
                print('Would move', p, '->', target)
            else:
                # Move the directory
                p.rename(target)
                if args.verbose:
                    print('Moved', p, '->', target)

    # Print summary report
    if args.verbose or args.dry_run:
        print('\nSummary:')
        print('  Publishable dirs:', [p.name for p in publishable])
        print('  Published datasets:', len(publishable_datasets))
        print('  Local datasets:', len(local_datasets))
        print('  Removed whitelisted dirs:', [str(p.relative_to(ROOT)) for p in removed_dirs])


if __name__ == '__main__':
    main()
